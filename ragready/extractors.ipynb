{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9526c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kwadw.DESKTOP-T9BSTPE\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#  extractors.py  –  Unified text/metadata extractors\n",
    "# ======================================================================\n",
    "#\n",
    "#  ### Iterator cheat-sheet\n",
    "#  | Iterator              | What it yields                                                      |\n",
    "#  | --------------------- | ------------------------------------------------------------------- |\n",
    "#  | **git_repo_iter**     | Every text/config/code file in a GitHub / GitLab repo               |\n",
    "#  | **confluence_iter**   | Every page in one or more Confluence spaces                         |\n",
    "#  | **website_iter**      | All pages in a website (BFS within same domain, optional depth)     |\n",
    "#  | **local_iter**        | Markdown from PDFs, DOCX, PPTX, XLSX, TXT, HTML, CSV, images,       |\n",
    "#  |                     | audio, ZIPs, EPUB, JSON, XML, and more (recursive, via MarkItDown)   |\n",
    "#\n",
    "#  Power by MarkItDown:\n",
    "#    - PDF, Word, PowerPoint, Excel, CSV, JSON, XML\n",
    "#    - Images (EXIF + OCR), Audio (EXIF + transcription)\n",
    "#    - HTML, Markdown, plain text, ZIPs (all inner contents), EPUB\n",
    "#    - YouTube URLs (with transcript/extraction)\n",
    "#\n",
    "#  Optional installs\n",
    "#  -----------------\n",
    "#  # pip install markitdown\n",
    "#  # pip install gitpython requests pandas tqdm\n",
    "#  # pip install atlassian-python-api beautifulsoup4 lxml\n",
    "#  # pip install pdfplumber python-docx python-pptx\n",
    "#\n",
    "#  Env vars for secrets (examples):\n",
    "#     GITHUB_TOKEN, GITLAB_TOKEN, CONFLUENCE_TOKEN, CONF_USER, CONF_URL\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Iterable, Optional, Set, Union, List, Dict, Any\n",
    "from pathlib import Path\n",
    "from datetime import datetime, UTC, timezone\n",
    "from time import strftime\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re, tempfile, os, shutil, requests, pandas as pd\n",
    "import tempfile, shutil, requests, os\n",
    "from git import Repo, InvalidGitRepositoryError, GitCommandError\n",
    "from atlassian import Confluence\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "import pdfplumber, docx\n",
    "from markitdown import MarkItDown\n",
    "from pptx import Presentation\n",
    "import re\n",
    "\n",
    "\n",
    "# ──────────────────────────  common record  ──────────────────────────\n",
    "@dataclass\n",
    "class DocumentRecord:\n",
    "    source: str\n",
    "    filename: str\n",
    "    title: str          | None\n",
    "    description: str    | None\n",
    "    author: str         | None\n",
    "    content: str\n",
    "    path_or_id: str     | None\n",
    "    url: str            | None\n",
    "    last_modified: str  | None\n",
    "    size_bytes: int     | None\n",
    "    def to_dict(self): return asdict(self)\n",
    "\n",
    "\n",
    "# ════════════════════════════  GIT REPOS  ════════════════════════════\n",
    "\n",
    "# ───────────────────── helper – robust “clean & clone” ─────────────────────\n",
    "def _prepare_repo_dir(clone_url: str, repo_dir: Path) -> Repo:\n",
    "    \"\"\"\n",
    "    Ensure *repo_dir* is a fresh, valid Git repo.\n",
    "\n",
    "    • If it's already a healthy repo → reuse it.\n",
    "    • If it's corrupted → move it aside and reclone.\n",
    "    • Windows-safe: handles file-locks and existing “_old” dirs.\n",
    "    \"\"\"\n",
    "    if repo_dir.exists():\n",
    "        try:\n",
    "            return Repo(repo_dir) \n",
    "        except InvalidGitRepositoryError:\n",
    "            print(f\"[info] {repo_dir} is broken; refreshing…\")\n",
    "\n",
    "            # Build a unique “…_old” path\n",
    "            alt = repo_dir.with_name(repo_dir.name + \"_old\")\n",
    "            if alt.exists():\n",
    "                # If stale, nuke it first\n",
    "                shutil.rmtree(alt, ignore_errors=True)\n",
    "\n",
    "            try:\n",
    "                repo_dir.rename(alt)\n",
    "            except (PermissionError, FileExistsError):\n",
    "                # Last-resort uniqueness\n",
    "                ts_alt = repo_dir.with_name(\n",
    "                    f\"{repo_dir.name}_old_{datetime.now(UTC).strftime('%Y%m%d%H%M%S')}\"\n",
    "                )\n",
    "                try:\n",
    "                    repo_dir.rename(ts_alt)\n",
    "                except Exception as e:\n",
    "                    print(f\"[warn] rename failed ({e}); force-deleting\")\n",
    "                    shutil.rmtree(repo_dir, ignore_errors=True)\n",
    "\n",
    "    # Now the path is definitely free\n",
    "    return Repo.clone_from(clone_url, repo_dir, depth=1, single_branch=True)\n",
    "\n",
    "\n",
    "# ───────────────────────────────  main iterator  ───────────────────────────\n",
    "def git_repo_iter(\n",
    "    repo_url: str,\n",
    "    token: Optional[str] = None,\n",
    "    *,\n",
    "    clone_dir: Union[str, Path] = Path(tempfile.gettempdir()) / \"git-mirror\",\n",
    "    include_ext: Optional[Set[str]] = None,\n",
    "    delete_after: bool = True,\n",
    ") -> Iterable[DocumentRecord]:\n",
    "    \"\"\"\n",
    "    Stream `DocumentRecord` for every text/config file in a GitHub or GitLab repo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    repo_url : str\n",
    "        HTTPS clone URL (`…github.com/owner/repo.git` or `…gitlab.com/group/repo.git`)\n",
    "    token : str | None\n",
    "        Personal-access token; omit for public repos.\n",
    "    clone_dir : Path\n",
    "        Parent folder under which a temp checkout is created.\n",
    "    include_ext : set[str] | None\n",
    "        File extensions to include (defaults to common text/code types).\n",
    "    delete_after : bool\n",
    "        Remove the temp repo folder once iteration completes.\n",
    "    \"\"\"\n",
    "    include_ext = include_ext or {\n",
    "        \".md\", \".markdown\", \".txt\",\n",
    "        \".py\", \".ipynb\", \".r\", \".R\", \".sh\",\n",
    "        \".yaml\", \".yml\", \".json\", \".toml\", \".ini\", \".cfg\", \".conf\",\n",
    "        \".csv\", \".tsv\", \".sql\",\n",
    "        \".html\", \".htm\", \".xml\", \".rst\",\n",
    "    }\n",
    "\n",
    "    source = (\n",
    "        \"github\"  if \"github.com\"  in repo_url else\n",
    "        \"gitlab\"  if \"gitlab.com\"  in repo_url else\n",
    "        \"git\"\n",
    "    )\n",
    "\n",
    "    clone_dir = Path(clone_dir).expanduser()\n",
    "    repo_name = repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
    "    repo_dir  = clone_dir / repo_name\n",
    "\n",
    "    # Build auth-injected URL when a token is supplied\n",
    "    clone_url = repo_url\n",
    "    if token and repo_url.startswith(\"https://\"):\n",
    "        if \"github.com\" in repo_url:\n",
    "            clone_url = repo_url.replace(\"https://\", f\"https://{token}@\")\n",
    "        elif \"gitlab.com\" in repo_url:\n",
    "            clone_url = repo_url.replace(\"https://\", f\"https://oauth2:{token}@\")\n",
    "\n",
    "    # Clone or refresh corrupted checkouts\n",
    "    try:\n",
    "        repo = _prepare_repo_dir(clone_url, repo_dir)\n",
    "    except GitCommandError as err:\n",
    "        print(f\"[error] git clone failed for {repo_url}: {err}\")\n",
    "        return  # abort iteration gracefully\n",
    "\n",
    "    repo_descr = _repo_description(repo_url, token)\n",
    "\n",
    "    try:\n",
    "        for path in repo_dir.rglob(\"*\"):\n",
    "            if not (path.is_file() and path.suffix.lower() in include_ext):\n",
    "                continue\n",
    "            try:\n",
    "                text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[warn] skip {path}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            author = _file_author(repo, path)\n",
    "            base   = repo_url.replace(\".git\", \"\")\n",
    "            view   = (\n",
    "                f\"{base}/-/blob/main/{path.relative_to(repo_dir)}\"\n",
    "                if \"gitlab.com\" in repo_url\n",
    "                else f\"{base}/blob/main/{path.relative_to(repo_dir)}\"\n",
    "            )\n",
    "\n",
    "            yield DocumentRecord(\n",
    "                source=source,\n",
    "                filename=path.name,\n",
    "                title=path.stem,\n",
    "                description=repo_descr,\n",
    "                author=author,\n",
    "                content=text,\n",
    "                path_or_id=str(path.relative_to(repo_dir)),\n",
    "                url=view,\n",
    "                last_modified=datetime.fromtimestamp(\n",
    "                    path.stat().st_mtime, tz=UTC\n",
    "                ).isoformat(),\n",
    "                size_bytes=path.stat().st_size,\n",
    "            )\n",
    "    finally:\n",
    "        if delete_after and repo_dir.exists():\n",
    "            shutil.rmtree(repo_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "# ────────────────────────────  helper functions  ──────────────────────────\n",
    "def _repo_description(repo_url: str, token: Optional[str]) -> Optional[str]:\n",
    "    try:\n",
    "        hdr = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "        if \"github.com\" in repo_url:\n",
    "            owner_repo = repo_url.replace(\"https://github.com/\", \"\").replace(\".git\", \"\")\n",
    "            r = requests.get(f\"https://api.github.com/repos/{owner_repo}\", headers=hdr, timeout=15)\n",
    "            return r.json().get(\"description\") if r.ok else None\n",
    "        if \"gitlab.com\" in repo_url:\n",
    "            pth = repo_url.replace(\"https://gitlab.com/\", \"\").replace(\".git\", \"\")\n",
    "            api = f\"https://gitlab.com/api/v4/projects/{requests.utils.quote(pth, safe='')}\"\n",
    "            r = requests.get(api, headers=hdr, timeout=15)\n",
    "            return r.json().get(\"description\") if r.ok else None\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _file_author(repo: Repo, fp: Path) -> Optional[str]:\n",
    "    try:\n",
    "        rel = str(fp.relative_to(repo.working_tree_dir))\n",
    "        commit = next(repo.iter_commits(paths=rel, max_count=1), None)\n",
    "        return commit.author.name if commit else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ════════════════════════════  CONFLUENCE  ════════════════════════════\n",
    "def confluence_iter(\n",
    "    base_url: str,\n",
    "    username: str,\n",
    "    api_token: str,\n",
    "    space_keys: List[str],\n",
    "    *,\n",
    "    limit: int | None = None,\n",
    "    plain_text: bool = False,\n",
    ") -> Iterable[DocumentRecord]:\n",
    "    \"\"\"\n",
    "    Stream pages from Confluence Cloud / DC.\n",
    "    \"\"\"\n",
    "    cf = Confluence(url=base_url, username=username, password=api_token)\n",
    "\n",
    "    for space in space_keys:\n",
    "        s_meta = cf.get_space(space, expand=\"description.plain\")\n",
    "        s_descr = (s_meta.get(\"description\", {}).get(\"plain\", {}).get(\"value\"))\n",
    "\n",
    "        start, fetched, size = 0, 0, 100\n",
    "        pbar = tqdm(desc=f\"[{space}] pages\", unit=\"page\")\n",
    "\n",
    "        while True:\n",
    "            pages = cf.get_all_pages_from_space(space, start=start, limit=size,\n",
    "                                                expand=\"version,body.storage\")\n",
    "            if not pages:\n",
    "                break\n",
    "            for p in pages:\n",
    "                fetched += 1; pbar.update(1)\n",
    "                if limit and fetched > limit:\n",
    "                    break\n",
    "\n",
    "                pid, html = p[\"id\"], p[\"body\"][\"storage\"][\"value\"]\n",
    "                cnt = BeautifulSoup(html, \"html.parser\").get_text(\"\\n\") if plain_text else html\n",
    "                yield DocumentRecord(\n",
    "                    source=\"confluence\",\n",
    "                    filename=f\"{space}-{pid}\",\n",
    "                    title=p[\"title\"],\n",
    "                    description=s_descr,\n",
    "                    author=p[\"version\"][\"by\"][\"displayName\"],\n",
    "                    content=cnt,\n",
    "                    path_or_id=pid,\n",
    "                    url=f\"{base_url.rstrip('/')}/pages/viewpage.action?pageId={pid}\",\n",
    "                    last_modified=datetime.fromisoformat(p[\"version\"][\"when\"]).astimezone(UTC).isoformat(),\n",
    "                    size_bytes=len(html.encode()),\n",
    "                )\n",
    "            if limit and fetched >= limit:\n",
    "                break\n",
    "            start += size\n",
    "        pbar.close()\n",
    "\n",
    "\n",
    "# ════════════════════════════  WEBSITE CRAWL  ═════════════════════════\n",
    "# ───────────────────── helper – read <meta> tags ─────────────────────\n",
    "def _meta(soup: BeautifulSoup, key: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the content of the first <meta name=\"<key>\" …> or\n",
    "    <meta property=\"og:<key>\" …> tag, if present.\n",
    "    \"\"\"\n",
    "    tag = (\n",
    "        soup.find(\"meta\", attrs={\"name\": key})\n",
    "        or soup.find(\"meta\", attrs={\"property\": f\"og:{key}\"})\n",
    "    )\n",
    "    return tag.get(\"content\", \"\").strip() if tag and tag.has_attr(\"content\") else None\n",
    "\n",
    "def website_iter(\n",
    "    roots: List[str],\n",
    "    *,\n",
    "    crawl_depth: int | None = None,\n",
    ") -> Iterable[DocumentRecord]:\n",
    "    \"\"\"\n",
    "    Breadth-first crawl within each domain and yield one `DocumentRecord`\n",
    "    per page.\n",
    "    \"\"\"\n",
    "    def _clean(txt: str) -> str:\n",
    "        txt = re.sub(r\"\\s+\\n\", \"\\n\", txt)\n",
    "        return re.sub(r\"\\n{2,}\", \"\\n\", txt).strip()\n",
    "\n",
    "    seen: set[str] = set()\n",
    "    for root in roots:\n",
    "        q: deque[tuple[str,int]] = deque([(root,0)])\n",
    "        host = urlparse(root).netloc\n",
    "\n",
    "        while q:\n",
    "            url, d = q.popleft()\n",
    "            if url in seen: continue\n",
    "            seen.add(url)\n",
    "\n",
    "            try:\n",
    "                html = requests.get(url, timeout=15).text\n",
    "            except Exception:\n",
    "                continue\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "            yield DocumentRecord(\n",
    "                source=host,\n",
    "                filename=url,\n",
    "                title=soup.title.string.strip() if soup.title else None,\n",
    "                description=_meta(soup, \"description\"),\n",
    "                author=_meta(soup, \"author\"),\n",
    "                content=_clean(soup.get_text(\" \", strip=True)),\n",
    "                path_or_id=None,\n",
    "                url=url,\n",
    "                last_modified=None,\n",
    "                size_bytes=len(html.encode()),\n",
    "            )\n",
    "\n",
    "            if crawl_depth is None or d < crawl_depth:\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    nxt = urljoin(url, a[\"href\"].split(\"#\")[0])\n",
    "                    if urlparse(nxt).netloc == host and nxt not in seen:\n",
    "                        q.append((nxt, d+1))\n",
    "\n",
    "\n",
    "# ════════════════════════════  LOCAL FILES  ═══════════════════════════\n",
    "def local_iter(\n",
    "    paths: list[str | Path],\n",
    "    *,\n",
    "    include_ext: Optional[Set[str]] = None,\n",
    "    # new optional args for MarkItDown:\n",
    "    llm_client: Any = None,     # e.g. OpenAI(api_key=…)\n",
    "    llm_model: str | None = None,\n",
    ") -> Iterable[DocumentRecord]:\n",
    "    \"\"\"\n",
    "    Recursively crawl `paths` and yield a DocumentRecord for each file.\n",
    "    Uses MarkItDown to convert supported types to Markdown via an LLM,\n",
    "    falling back to per-extension extractors if that fails.\n",
    "    \"\"\"\n",
    "    # 1) Build your MarkItDown instance once\n",
    "    md = MarkItDown(\n",
    "        llm_client=llm_client,\n",
    "        llm_model=llm_model or \"gpt-4\"\n",
    "    )\n",
    "\n",
    "    # 2) Default extension filter (optional–you can override)\n",
    "    include_ext = include_ext or {\n",
    "        \".md\", \".markdown\", \".txt\",\n",
    "        \".py\", \".ipynb\", \".r\", \".R\", \".sh\",\n",
    "        \".yaml\", \".yml\", \".json\", \".toml\", \".ini\", \".cfg\", \".conf\",\n",
    "        \".csv\", \".tsv\", \".sql\",\n",
    "        \".html\", \".htm\", \".xml\", \".rst\",\n",
    "        \".pdf\", \".docx\", \".pptx\", \".xlsx\", \".xls\",\n",
    "        \".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\",\n",
    "        \".mp3\", \".wav\", \".ogg\",\n",
    "        \".zip\", \".epub\"\n",
    "    }\n",
    "\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if p.is_dir():\n",
    "            for f in p.rglob(\"*\"):\n",
    "                if f.suffix.lower() in include_ext:\n",
    "                    recs = _read_local(f, md)\n",
    "                    if recs:\n",
    "                        # handle multi‐record (e.g. ZIP) vs single\n",
    "                        if isinstance(recs, Iterable) and not isinstance(recs, (str, DocumentRecord)):\n",
    "                            for rec in recs:\n",
    "                                yield rec\n",
    "                        else:\n",
    "                            yield recs\n",
    "\n",
    "        elif p.exists() and p.suffix.lower() in include_ext:\n",
    "            recs = _read_local(p, md)\n",
    "            if recs:\n",
    "                if isinstance(recs, Iterable) and not isinstance(recs, (str, DocumentRecord)):\n",
    "                    for rec in recs:\n",
    "                        yield rec\n",
    "                else:\n",
    "                    yield recs\n",
    "\n",
    "        else:\n",
    "            print(f\"[warn] unsupported or missing: {p}\")\n",
    "            \n",
    "\n",
    "# --------------- local readers ----------------\n",
    "def _clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Collapse super-long whitespace / blank lines.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\\n\", \"\\n\", text)   # strip trailing spaces on each line\n",
    "    return re.sub(r\"\\n{2,}\", \"\\n\", text).strip()\n",
    "\n",
    "def _read_local(\n",
    "    path: Path,\n",
    "    md: MarkItDown\n",
    ") -> DocumentRecord | Iterable[DocumentRecord] | None:\n",
    "    \"\"\"\n",
    "    1) Try md.convert(...) to produce markdown via LLM\n",
    "    2) On failure, fall back to per-filetype extraction\n",
    "    \"\"\"\n",
    "    ext = path.suffix.lower()\n",
    "\n",
    "    # --- 1) Primary: MarkItDown via LLM ---\n",
    "    try:\n",
    "        result = md.convert(str(path))\n",
    "        # result.text_content holds the Markdown string\n",
    "        md_text = getattr(result, \"text_content\", result)\n",
    "\n",
    "        # ZIP/Archive support? If result is dict-like, combine:\n",
    "        if isinstance(md_text, dict):\n",
    "            combined = \"\\n\\n\".join(\n",
    "                f\"### {fname}\\n\\n{content}\"\n",
    "                for fname, content in md_text.items()\n",
    "            )\n",
    "            return DocumentRecord(\n",
    "                source=\"local\",\n",
    "                filename=path.name,\n",
    "                title=None, description=None, author=None,\n",
    "                content=combined,\n",
    "                path_or_id=str(path),\n",
    "                url=None,\n",
    "                last_modified=None,\n",
    "                size_bytes=path.stat().st_size\n",
    "            )\n",
    "\n",
    "        return DocumentRecord(\n",
    "            source=\"local\",\n",
    "            filename=path.name,\n",
    "            title=None, description=None, author=None,\n",
    "            content=md_text,\n",
    "            path_or_id=str(path),\n",
    "            url=None,\n",
    "            last_modified=None,\n",
    "            size_bytes=path.stat().st_size\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] MarkItDown failed for {path}: {e}\")\n",
    "\n",
    "    # --- 2) Fallback: per‐extension extractor ---\n",
    "    try:\n",
    "        if ext == \".pdf\":\n",
    "            with pdfplumber.open(path) as pdf:\n",
    "                meta = pdf.metadata or {}\n",
    "                txt  = \"\\n\".join(p.extract_text() or \"\" for p in pdf.pages)\n",
    "            return DocumentRecord(\n",
    "                \"local\", path.name, meta.get(\"Title\"), None, meta.get(\"Author\"),\n",
    "                _clean(txt), str(path), None,\n",
    "                _pdf_date(meta.get(\"CreationDate\")), path.stat().st_size\n",
    "            )\n",
    "\n",
    "        if ext == \".docx\":\n",
    "            doc  = docx.Document(path)\n",
    "            core = doc.core_properties\n",
    "            txt  = \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "            return DocumentRecord(\n",
    "                \"local\", path.name, core.title, None, core.author,\n",
    "                _clean(txt), str(path), None,\n",
    "                _py_dt(core.created), path.stat().st_size\n",
    "            )\n",
    "\n",
    "        if ext == \".pptx\":\n",
    "            prs   = Presentation(path)\n",
    "            core  = prs.core_properties\n",
    "            slides_txt = [\n",
    "                shape.text for sl in prs.slides for shape in sl.shapes\n",
    "                if hasattr(shape, \"text\")\n",
    "            ]\n",
    "            return DocumentRecord(\n",
    "                \"local\", path.name, core.title, None, core.author,\n",
    "                _clean(\"\\n\".join(slides_txt)), str(path), None,\n",
    "                _py_dt(core.created), path.stat().st_size\n",
    "            )\n",
    "\n",
    "        if ext in {\".html\", \".htm\"}:\n",
    "            soup = BeautifulSoup(path.read_text(encoding=\"utf-8\", errors=\"ignore\"), \"lxml\")\n",
    "            return DocumentRecord(\n",
    "                \"local\", path.name,\n",
    "                soup.title.string.strip() if soup.title else None,\n",
    "                None, _meta(soup, \"author\"),\n",
    "                _clean(soup.get_text(\" \", strip=True)), str(path), None,\n",
    "                None, path.stat().st_size\n",
    "            )\n",
    "\n",
    "        if ext in {\n",
    "            \".txt\", \".md\", \".markdown\", \".rst\", \".py\", \".sh\", \".r\", \".R\",\n",
    "            \".yaml\", \".yml\", \".json\", \".toml\", \".ini\", \".cfg\", \".conf\",\n",
    "            \".csv\", \".tsv\", \".sql\", \".xml\"\n",
    "        }:\n",
    "            txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            return DocumentRecord(\n",
    "                \"local\", path.name, None, None, None,\n",
    "                _clean(txt), str(path), None,\n",
    "                None, path.stat().st_size\n",
    "            )\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"[warn] fallback extractor failed for {path}: {ex}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# from markitdown import MarkItDown\n",
    "\n",
    "# # e.g. pass in your OpenAI client and desired model\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=\"YOUR_KEY\")\n",
    "\n",
    "# for doc in local_iter(\n",
    "#     [\"/path/to/folder\", \"/another/file.pdf\"],\n",
    "#     llm_client=client,\n",
    "#     llm_model=\"gpt-4o\",\n",
    "# ):\n",
    "#     print(doc.filename, doc.content[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309fb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Example usage  (only runs when you `python extractors.py`)\n",
    "# ----------------------------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     import pandas as pd\n",
    "#     import os\n",
    "#     from pathlib import Path\n",
    "\n",
    "#     ### 1) GitHub / GitLab repos\n",
    "#     urls = [\n",
    "#         \"https://github.com/pandas-dev/pandas.git\",\n",
    "#         \"https://gitlab.com/knowusuboaky-group/knowusuboaky-project.git\",\n",
    "#     ]\n",
    "#     git_rows = [rec.to_dict() for url in urls for rec in git_repo_iter(url)]\n",
    "#     git_df = pd.DataFrame(git_rows)\n",
    "#     print(\"\\nGit repos preview:\")\n",
    "#     print(git_df[[\"source\", \"filename\", \"author\", \"url\"]].head())\n",
    "\n",
    "    # ### 2) Confluence spaces  (requires CONFLUENCE_TOKEN env var)\n",
    "    # conf_rows = []\n",
    "    # if os.getenv(\"CONFLUENCE_TOKEN\"):\n",
    "    #     conf_rows = [\n",
    "    #         rec.to_dict()\n",
    "    #         for rec in confluence_iter(\n",
    "    #             base_url=os.getenv(\"CONF_URL\",  \"https://your-domain.atlassian.net/wiki\"),\n",
    "    #             username=os.getenv(\"CONF_USER\", \"you@example.com\"),\n",
    "    #             api_token=os.getenv(\"CONFLUENCE_TOKEN\"),\n",
    "    #             space_keys=[\"ENG\"],        # list as many spaces as you like\n",
    "    #             limit=10,                  # fetch first 10 pages for demo\n",
    "    #             plain_text=True,\n",
    "    #         )\n",
    "    #     ]\n",
    "    #     conf_df = pd.DataFrame(conf_rows)\n",
    "    #     print(\"\\nConfluence preview:\")\n",
    "    #     print(conf_df[[\"filename\", \"author\", \"url\"]].head())\n",
    "\n",
    "    # ### 3) Website crawl\n",
    "    # web_rows = [\n",
    "    #     rec.to_dict()\n",
    "    #     for rec in website_iter(\n",
    "    #         roots=[\"https://www.python.org\"],\n",
    "    #         crawl_depth=1,\n",
    "    #     )\n",
    "    # ]\n",
    "    # web_df = pd.DataFrame(web_rows)\n",
    "    # print(\"\\nWebsite preview:\")\n",
    "    # print(web_df[[\"source\", \"title\", \"url\"]].head())\n",
    "\n",
    "    # ### 4) Local documents\n",
    "    # from pathlib import Path\n",
    "    # import pandas as pd\n",
    "    # from markitdown import MarkItDown          # already imported inside local_iter\n",
    "    # # from openai import OpenAI                # only if you’re passing an LLM client\n",
    "\n",
    "    # # Optional LLM client (set to None if you don't need the LLM conversion layer):\n",
    "    # # client = OpenAI(api_key=\"YOUR_KEY\")\n",
    "    # client = None       \n",
    "    # llm_model = None\n",
    "\n",
    "    # # 1️⃣  Run the iterator and stash the results\n",
    "    # docs = [\n",
    "    #     rec.to_dict()                          # turn each DocumentRecord into a plain dict\n",
    "    #     for rec in local_iter(\n",
    "    #         [\"ragready_test.docx\", \"ragready_test.xlsx\"],\n",
    "    #         llm_client=client,                 # can stay None for pure local parsing\n",
    "    #         llm_model=llm_model                # or \"gpt-4o\" / \"gpt-4\" etc.\n",
    "    #     )\n",
    "    # ]\n",
    "\n",
    "    # # 2️⃣  Build a DataFrame\n",
    "    # df = pd.DataFrame(docs)\n",
    "\n",
    "    # # 3️⃣  Inspect or save\n",
    "    # print(df.head())         # quick peek\n",
    "    # # df.to_csv(\"ragready_preview.csv\", index=False)   # optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ece238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Website preview:\n",
      "           source                          title  \\\n",
      "0  www.python.org          Welcome to Python.org   \n",
      "1  www.python.org          Welcome to Python.org   \n",
      "2  www.python.org     Python Software Foundation   \n",
      "3  www.python.org  Python Job Board | Python.org   \n",
      "4  www.python.org     Our Community | Python.org   \n",
      "\n",
      "                                 url  \n",
      "0             https://www.python.org  \n",
      "1            https://www.python.org/  \n",
      "2        https://www.python.org/psf/  \n",
      "3       https://www.python.org/jobs/  \n",
      "4  https://www.python.org/community/  \n"
     ]
    }
   ],
   "source": [
    "### 3) Website crawl\n",
    "web_rows = [\n",
    "    rec.to_dict()\n",
    "    for rec in website_iter(\n",
    "        roots=[\"https://www.python.org\"],\n",
    "        crawl_depth=1,\n",
    "    )\n",
    "]\n",
    "web_df = pd.DataFrame(web_rows)\n",
    "print(\"\\nWebsite preview:\")\n",
    "print(web_df[[\"source\", \"title\", \"url\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9234157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] C:\\Users\\KWADW~1.DES\\AppData\\Local\\Temp\\git-mirror\\knowusuboaky-project is broken; refreshing…\n",
      "\n",
      "Git repos preview:\n",
      "   source                 filename            author  \\\n",
      "0  github       .devcontainer.json  Matthew Roeschke   \n",
      "1  github              .gitpod.yml  Matthew Roeschke   \n",
      "2  github  .pre-commit-config.yaml  Matthew Roeschke   \n",
      "3  github               AUTHORS.md  Matthew Roeschke   \n",
      "4  github              codecov.yml  Matthew Roeschke   \n",
      "\n",
      "                                                 url  \n",
      "0  https://github.com/pandas-dev/pandas/blob/main...  \n",
      "1  https://github.com/pandas-dev/pandas/blob/main...  \n",
      "2  https://github.com/pandas-dev/pandas/blob/main...  \n",
      "3  https://github.com/pandas-dev/pandas/blob/main...  \n",
      "4  https://github.com/pandas-dev/pandas/blob/main...  \n"
     ]
    }
   ],
   "source": [
    "### 1) GitHub / GitLab repos\n",
    "urls = [\n",
    "    \"https://github.com/pandas-dev/pandas.git\",\n",
    "    \"https://gitlab.com/knowusuboaky-group/knowusuboaky-project.git\",\n",
    "]\n",
    "git_rows = [rec.to_dict() for url in urls for rec in git_repo_iter(url)]\n",
    "git_df = pd.DataFrame(git_rows)\n",
    "print(\"\\nGit repos preview:\")\n",
    "print(git_df[[\"source\", \"filename\", \"author\", \"url\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc9379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source            filename title description author  \\\n",
      "0  local  ragready_test.docx  None        None   None   \n",
      "1  local  ragready_test.xlsx  None        None   None   \n",
      "\n",
      "                                             content          path_or_id  \\\n",
      "0  # RAGready Test Document\\n\\nThis Word file was...  ragready_test.docx   \n",
      "1  ## Sheet1\\n| id | name | value |\\n| --- | --- ...  ragready_test.xlsx   \n",
      "\n",
      "    url last_modified  size_bytes  \n",
      "0  None          None       36710  \n",
      "1  None          None        5468  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9679c461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "path_or_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "last_modified",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "size_bytes",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9163cf45-2ff8-4677-bf66-c42d5b9dd68b",
       "rows": [
        [
         "0",
         "local",
         "ragready_test.docx",
         null,
         null,
         null,
         "# RAGready Test Document\n\nThis Word file was generated automatically for testing the local\\_iter extractor in RAGready.\n\nFeel free to modify or extend this document as needed.",
         "ragready_test.docx",
         null,
         null,
         "36710"
        ],
        [
         "1",
         "local",
         "ragready_test.xlsx",
         null,
         null,
         null,
         "## Sheet1\n| id | name | value |\n| --- | --- | --- |\n| 1 | Alpha | 10.50 |\n| 2 | Beta | 23.00 |\n| 3 | Gamma | 17.75 |",
         "ragready_test.xlsx",
         null,
         null,
         "5468"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>path_or_id</th>\n",
       "      <th>url</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>size_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>local</td>\n",
       "      <td>ragready_test.docx</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td># RAGready Test Document\\n\\nThis Word file was...</td>\n",
       "      <td>ragready_test.docx</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>36710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local</td>\n",
       "      <td>ragready_test.xlsx</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>## Sheet1\\n| id | name | value |\\n| --- | --- ...</td>\n",
       "      <td>ragready_test.xlsx</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source            filename title description author  \\\n",
       "0  local  ragready_test.docx  None        None   None   \n",
       "1  local  ragready_test.xlsx  None        None   None   \n",
       "\n",
       "                                             content          path_or_id  \\\n",
       "0  # RAGready Test Document\\n\\nThis Word file was...  ragready_test.docx   \n",
       "1  ## Sheet1\\n| id | name | value |\\n| --- | --- ...  ragready_test.xlsx   \n",
       "\n",
       "    url last_modified  size_bytes  \n",
       "0  None          None       36710  \n",
       "1  None          None        5468  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
